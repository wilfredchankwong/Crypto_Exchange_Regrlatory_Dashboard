{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "# Define the number of records as a variable\n",
    "num_records = 2000000\n",
    "num_records_user = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, FloatType, DateType\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# Number of files to save each DataFrame into\n",
    "# num_files = 10\n",
    "\n",
    "# Number of records you want per file\n",
    "num_records_per_file = 20000\n",
    "# Base path for saving files, replace with your desired path\n",
    "base_path = \"crypto_db\"\n",
    "\n",
    "# # Create SparkSession\n",
    "# spark = SparkSession.builder.appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User table\n",
    "countries = [\n",
    "    \"United States\",\n",
    "    \"Canada\",\n",
    "    \"United Kingdom\",\n",
    "    \"France\",\n",
    "    \"Germany\",\n",
    "    \"Italy\",\n",
    "    \"Spain\",\n",
    "    \"Netherlands\",\n",
    "    \"Belgium\",\n",
    "    \"Sweden\",\n",
    "    \"Norway\",\n",
    "    \"Denmark\",\n",
    "    \"Switzerland\",\n",
    "    \"Austria\",\n",
    "    \"Ireland\",\n",
    "    \"Portugal\",\n",
    "    \"Greece\",\n",
    "    \"Finland\",\n",
    "    \"Luxembourg\",\n",
    "    \"Iceland\",\n",
    "    \"Australia\",\n",
    "    \"New Zealand\",\n",
    "    \"China\"\n",
    "    # Add more countries as needed\n",
    "]\n",
    "\n",
    "user_data = {\n",
    "    'UserID': ['USR' + str(i) for i in range(1, num_records_user + 1)],\n",
    "    'UserName': [fake.user_name() for _ in range(num_records_user)],\n",
    "    'Email': [fake.email() for _ in range(num_records_user)],\n",
    "    'Password': [fake.password() for _ in range(num_records_user)],\n",
    "    'UserType': [np.random.choice(['Individual', 'Enterprise']) for _ in range(num_records_user)],\n",
    "    'RegistrationDate': [fake.date_between(start_date='-2y', end_date='today') for _ in range(num_records_user)],\n",
    "    'EnterpriseName': [fake.company() if np.random.rand() > 0.5 else None for _ in range(num_records_user)],\n",
    "    'Country': [random.choice(countries) for _ in range(num_records_user)]\n",
    "}\n",
    "users_df = pd.DataFrame(user_data)\n",
    "\n",
    "# Split the DataFrame into chunks and export each one to a CSV file\n",
    "total_records = len(users_df)  # Total number of records in the DataFrame\n",
    "num_files = (total_records + num_records_per_file - 1) // num_records_per_file  # Calculate the number of files needed\n",
    "\n",
    "directory = f'{base_path}/users'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_idx = i * num_records_per_file\n",
    "    end_idx = start_idx + num_records_per_file\n",
    "    chunk = users_df.iloc[start_idx:end_idx]\n",
    "    chunk.to_csv(f'{base_path}/users/users_{i+1:02d}.csv', index=False)\n",
    "\n",
    "# # Define schema for the DataFrame\n",
    "# schema = StructType([StructField('UserType', StringType(), True)])\n",
    "\n",
    "# # Create DataFrame with the specified schema\n",
    "# spark_df = spark.createDataFrame(users_df, schema=schema)\n",
    "\n",
    "# # save files\n",
    "# spark_df.write.mode(\"overwrite\").parquet(f\"{base_path}/user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiat table\n",
    "fiat_data = {\n",
    "    'FiatID': ['CUR' + str(i) for i in range(1, 31)],\n",
    "    'Name': [fake.currency_name() for _ in range(30)],\n",
    "    'Symbol': [fake.currency_code() for _ in range(30)],\n",
    "}\n",
    "fiat_df = pd.DataFrame(fiat_data)\n",
    "\n",
    "# Split the DataFrame into chunks and export each one to a CSV file\n",
    "total_records = len(fiat_df)  # Total number of records in the DataFrame\n",
    "num_files = (total_records + num_records_per_file - 1) // num_records_per_file  # Calculate the number of files needed\n",
    "\n",
    "directory = f'{base_path}/fiat'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_idx = i * num_records_per_file\n",
    "    end_idx = start_idx + num_records_per_file\n",
    "    chunk = fiat_df.iloc[start_idx:end_idx]\n",
    "    chunk.to_csv(f'{base_path}/fiat/fiat_{i+1:02d}.csv', index=False)\n",
    "\n",
    "\n",
    "# # Create DataFrame with the specified schema\n",
    "# spark_df = spark.createDataFrame(fiat_df)\n",
    "\n",
    "# spark_df.write.mode(\"overwrite\").parquet(f\"{base_path}/fiat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cryptocurrency table with real names and symbols\n",
    "crypto_data = {\n",
    "    'CryptocurrencyID': ['CRY' + str(i) for i in range(1, 11)],\n",
    "    'Name': ['Bitcoin', 'Ethereum', 'Ripple', 'Litecoin', 'Cardano', 'Polkadot', 'Bitcoin Cash', 'Stellar', 'Chainlink', 'Binance Coin'],\n",
    "    'Symbol': ['BTC', 'ETH', 'XRP', 'LTC', 'ADA', 'DOT', 'BCH', 'XLM', 'LINK', 'BNB']\n",
    "}\n",
    "crypto_df = pd.DataFrame(crypto_data)\n",
    "\n",
    "# Split the DataFrame into chunks and export each one to a CSV file\n",
    "total_records = len(crypto_df)  # Total number of records in the DataFrame\n",
    "num_files = (total_records + num_records_per_file - 1) // num_records_per_file  # Calculate the number of files needed\n",
    "\n",
    "directory = f'{base_path}/crypto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_idx = i * num_records_per_file\n",
    "    end_idx = start_idx + num_records_per_file\n",
    "    chunk = crypto_df.iloc[start_idx:end_idx]\n",
    "    chunk.to_csv(f'{base_path}/crypto/crypto_{i+1:02d}.csv', index=False)\n",
    "\n",
    "# # Create DataFrame with the specified schema\n",
    "# spark_df = spark.createDataFrame(crypto_df)\n",
    "\n",
    "# spark_df.write.mode(\"overwrite\").parquet(f\"{base_path}/crypto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FiatTransaction table\n",
    "FiatTransaction = {\n",
    "    'OrderID': ['FiatTrans' + str(i) for i in range(1, num_records + 1)],\n",
    "    'TransType': [np.random.choice(['Fiat Deposit', 'Fiat Withdrawl']) for _ in range(num_records)],\n",
    "    'UserID': ['USR' + str(np.random.randint(1, num_records_user + 1)) for _ in range(num_records)],\n",
    "    # 'CryptocurrencyID': ['CRY' + str(np.random.randint(1, 11)) for _ in range(num_records)],\n",
    "    # 'CryptoAmount': [round(np.random.uniform(0.1, 100), 2) for _ in range(num_records)],\n",
    "    'FiatID': ['CUR' + str(np.random.randint(1, 31)) for _ in range(num_records)],\n",
    "    'FiatAmount': [round(np.random.uniform(100, 10000), 2) for _ in range(num_records)],\n",
    "    'Timestamp': [fake.date_time_between(start_date='-1y', end_date='now') for _ in range(num_records)],\n",
    "}\n",
    "FiatTransaction_df = pd.DataFrame(FiatTransaction)\n",
    "\n",
    "# Split the DataFrame into chunks and export each one to a CSV file\n",
    "total_records = len(FiatTransaction_df)  # Total number of records in the DataFrame\n",
    "num_files = (total_records + num_records_per_file - 1) // num_records_per_file  # Calculate the number of files needed\n",
    "\n",
    "directory = f'{base_path}/FiatTransaction'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_idx = i * num_records_per_file\n",
    "    end_idx = start_idx + num_records_per_file\n",
    "    chunk = FiatTransaction_df.iloc[start_idx:end_idx]\n",
    "    chunk.to_csv(f'{base_path}/FiatTransaction/FiatTransaction_{i+1:02d}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CryptoTransaction table\n",
    "CryptoTransaction = {\n",
    "    'OrderID': ['CryTrans' + str(i) for i in range(1, num_records + 1)],\n",
    "    'TransType': [np.random.choice(['Crypto Deposit', 'Crypto Withdrawl']) for _ in range(num_records)],\n",
    "    'UserID': ['USR' + str(np.random.randint(1, num_records_user + 1)) for _ in range(num_records)],\n",
    "    'CryptocurrencyID': ['CRY' + str(np.random.randint(1, 11)) for _ in range(num_records)],\n",
    "    'CryptoAmount': [round(np.random.uniform(0.1, 100), 2) for _ in range(num_records)],\n",
    "    'Timestamp': [fake.date_time_between(start_date='-1y', end_date='now') for _ in range(num_records)],\n",
    "}\n",
    "CryptoTransaction_df = pd.DataFrame(CryptoTransaction)\n",
    "\n",
    "# Split the DataFrame into chunks and export each one to a CSV file\n",
    "total_records = len(CryptoTransaction_df)  # Total number of records in the DataFrame\n",
    "num_files = (total_records + num_records_per_file - 1) // num_records_per_file  # Calculate the number of files needed\n",
    "\n",
    "directory = f'{base_path}/CryptoTransaction'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_idx = i * num_records_per_file\n",
    "    end_idx = start_idx + num_records_per_file\n",
    "    chunk = CryptoTransaction_df.iloc[start_idx:end_idx]\n",
    "    chunk.to_csv(f'{base_path}/CryptoTransaction/CryptoTransaction_{i+1:02d}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpotTrade table\n",
    "spot_trade_data = {\n",
    "    'OrderID': ['SPT' + str(i) for i in range(1, num_records + 1)],\n",
    "    'OrderType': [np.random.choice(['Buy', 'Sell']) for _ in range(num_records)],\n",
    "    'UserID': ['USR' + str(np.random.randint(1, num_records_user + 1)) for _ in range(num_records)],\n",
    "    'CryptocurrencyID': ['CRY' + str(np.random.randint(1, 11)) for _ in range(num_records)],\n",
    "    'CryptoAmount': [round(np.random.uniform(0.1, 100), 2) for _ in range(num_records)],\n",
    "    'FiatAmount': [round(np.random.uniform(100, 10000), 2) for _ in range(num_records)],\n",
    "    'FiatID': ['CUR' + str(np.random.randint(1, 31)) for _ in range(num_records)],\n",
    "    'Timestamp': [fake.date_time_between(start_date='-1y', end_date='now') for _ in range(num_records)],\n",
    "}\n",
    "spot_trades_df = pd.DataFrame(spot_trade_data)\n",
    "\n",
    "# Split the DataFrame into chunks and export each one to a CSV file\n",
    "total_records = len(spot_trades_df)  # Total number of records in the DataFrame\n",
    "num_files = (total_records + num_records_per_file - 1) // num_records_per_file  # Calculate the number of files needed\n",
    "\n",
    "directory = f'{base_path}/spot_trades'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_idx = i * num_records_per_file\n",
    "    end_idx = start_idx + num_records_per_file\n",
    "    chunk = spot_trades_df.iloc[start_idx:end_idx]\n",
    "    chunk.to_csv(f'{base_path}/spot_trades/spot_trades_{i+1:02d}.csv', index=False)\n",
    "\n",
    "# # Define schema for the DataFrame\n",
    "# schema = StructType([StructField('OrderType', StringType(), True)])\n",
    "\n",
    "# # Create DataFrame with the specified schema\n",
    "# spark_df = spark.createDataFrame(spot_trades_df, schema=schema)\n",
    "\n",
    "# spark_df.write.mode(\"overwrite\").parquet(f\"{base_path}/spot_trades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpotTrade table\n",
    "spot_trade_data = {\n",
    "    'OrderID': ['SPT' + str(i) for i in range(1, num_records + 1)],\n",
    "    'OrderType': [np.random.choice(['Buy', 'Sell']) for _ in range(num_records)],\n",
    "    'UserID': ['USR' + str(np.random.randint(1, num_records_user + 1)) for _ in range(num_records)],\n",
    "    'CryptocurrencyID': ['CRY' + str(np.random.randint(1, 11)) for _ in range(num_records)],\n",
    "    'CryptoAmount': [round(np.random.uniform(0.1, 100), 2) for _ in range(num_records)],\n",
    "    'FiatAmount': [round(np.random.uniform(100, 10000), 2) for _ in range(num_records)],\n",
    "    'FiatID': ['CUR' + str(np.random.randint(1, 31)) for _ in range(num_records)],\n",
    "    'Timestamp': [fake.date_time_between(start_date='-1y', end_date='now') for _ in range(num_records)],\n",
    "}\n",
    "spot_trades_df = pd.DataFrame(spot_trade_data)\n",
    "\n",
    "# Split the DataFrame into chunks and export each one to a CSV file\n",
    "total_records = len(spot_trades_df)  # Total number of records in the DataFrame\n",
    "num_files = (total_records + num_records_per_file - 1) // num_records_per_file  # Calculate the number of files needed\n",
    "\n",
    "directory = f'{base_path}/spot_trades'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_idx = i * num_records_per_file\n",
    "    end_idx = start_idx + num_records_per_file\n",
    "    chunk = spot_trades_df.iloc[start_idx:end_idx]\n",
    "    chunk.to_csv(f'{base_path}/spot_trades/spot_trades_{i+1:02d}.csv', index=False)\n",
    "\n",
    "# # Define schema for the DataFrame\n",
    "# schema = StructType([StructField('OrderType', StringType(), True)])\n",
    "\n",
    "# # Create DataFrame with the specified schema\n",
    "# spark_df = spark.createDataFrame(spot_trades_df, schema=schema)\n",
    "\n",
    "# spark_df.write.mode(\"overwrite\").parquet(f\"{base_path}/spot_trades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTCTrade table\n",
    "otc_trade_data = {\n",
    "    'OrderID': ['OTC' + str(i) for i in range(1, num_records + 1)],\n",
    "    'OrderType': [np.random.choice(['Buy', 'Sell']) for _ in range(num_records)],\n",
    "    'SellerUserID': ['USR' + str(np.random.randint(1, num_records_user + 1)) for _ in range(num_records)],\n",
    "    'BuyerUserID': ['USR' + str(np.random.randint(1, num_records_user + 1)) for _ in range(num_records)],\n",
    "    'CryptocurrencyID': ['CRY' + str(np.random.randint(1, 11)) for _ in range(num_records)],\n",
    "    'CryptoAmount': [round(np.random.uniform(0.1, 100), 2) for _ in range(num_records)],\n",
    "    'FiatAmount': [round(np.random.uniform(100, 10000), 2) for _ in range(num_records)],\n",
    "    'FiatID': ['CUR' + str(np.random.randint(1, 31)) for _ in range(num_records)],\n",
    "    'Timestamp': [fake.date_time_between(start_date='-1y', end_date='now') for _ in range(num_records)],\n",
    "}\n",
    "otc_trades_df = pd.DataFrame(otc_trade_data)\n",
    "\n",
    "# Split the DataFrame into chunks and export each one to a CSV file\n",
    "total_records = len(otc_trades_df)  # Total number of records in the DataFrame\n",
    "num_files = (total_records + num_records_per_file - 1) // num_records_per_file  # Calculate the number of files needed\n",
    "\n",
    "directory = f'{base_path}/otc_trades'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_idx = i * num_records_per_file\n",
    "    end_idx = start_idx + num_records_per_file\n",
    "    chunk = otc_trades_df.iloc[start_idx:end_idx]\n",
    "    chunk.to_csv(f'{base_path}/otc_trades/otc_trades_{i+1:02d}.csv', index=False)\n",
    "\n",
    "# # Define schema for the DataFrame\n",
    "# schema = StructType([StructField('OrderType', StringType(), True)])\n",
    "\n",
    "# # Create DataFrame with the specified schema\n",
    "# spark_df = spark.createDataFrame(otc_trades_df, schema=schema)\n",
    "\n",
    "# spark_df.write.mode(\"overwrite\").parquet(f\"{base_path}/otc_trades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FuturesTrade table\n",
    "futures_trade_data = {\n",
    "    'TradeID': ['FUT' + str(i) for i in range(1, num_records + 1)],\n",
    "    'UserID': ['USR' + str(np.random.randint(1, num_records_user + 1)) for _ in range(num_records)],\n",
    "    'ContractType': [np.random.choice(['Long', 'Short']) for _ in range(num_records)],\n",
    "    'CryptocurrencyID': ['CRY' + str(np.random.randint(1, 11)) for _ in range(num_records)],\n",
    "    'CryptoAmount': [round(np.random.uniform(0.1, 100), 2) for _ in range(num_records)],\n",
    "    'FiatID': ['CUR' + str(np.random.randint(1, 31)) for _ in range(num_records)],\n",
    "    'FiatAmount': [round(np.random.uniform(100, 10000), 2) for _ in range(num_records)],\n",
    "    'Timestamp': [fake.date_time_between(start_date='-1y', end_date='now') for _ in range(num_records)],\n",
    "}\n",
    "futures_trades_df = pd.DataFrame(futures_trade_data)\n",
    "\n",
    "# Split the DataFrame into chunks and export each one to a CSV file\n",
    "total_records = len(futures_trades_df)  # Total number of records in the DataFrame\n",
    "num_files = (total_records + num_records_per_file - 1) // num_records_per_file  # Calculate the number of files needed\n",
    "\n",
    "directory = f'{base_path}/futures_trades'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_idx = i * num_records_per_file\n",
    "    end_idx = start_idx + num_records_per_file\n",
    "    chunk = futures_trades_df.iloc[start_idx:end_idx]\n",
    "    chunk.to_csv(f'{base_path}/futures_trades/futures_trades_{i+1:02d}.csv', index=False)\n",
    "\n",
    "# # Define schema for the DataFrame\n",
    "# schema = StructType([StructField('ContractType', StringType(), True)])\n",
    "\n",
    "# # Create DataFrame with the specified schema\n",
    "# spark_df = spark.createDataFrame(futures_trades_df, schema=schema)\n",
    "\n",
    "# spark_df.write.mode(\"overwrite\").parquet(f\"{base_path}/futures_trades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KYCInformation table\n",
    "kyc_information_data = {\n",
    "    'KYCInfoID': ['KYC' + str(i) for i in range(1, num_records_user + 1)],\n",
    "    'UserID': ['USR' + str(np.random.randint(1, num_records_user + 1)) for _ in range(num_records_user)],\n",
    "    'VerificationStatus': [np.random.choice(['Pending', 'Approved', 'Rejected']) for _ in range(num_records_user)],\n",
    "    'VerificationDate': [fake.date_between(start_date='-2y', end_date='today') for _ in range(num_records_user)],\n",
    "}\n",
    "kyc_information_df = pd.DataFrame(kyc_information_data)\n",
    "\n",
    "# Split the DataFrame into chunks and export each one to a CSV file\n",
    "total_records = len(kyc_information_df)  # Total number of records in the DataFrame\n",
    "num_files = (total_records + num_records_per_file - 1) // num_records_per_file  # Calculate the number of files needed\n",
    "\n",
    "directory = f'{base_path}/kyc_information'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_idx = i * num_records_per_file\n",
    "    end_idx = start_idx + num_records_per_file\n",
    "    chunk = kyc_information_df.iloc[start_idx:end_idx]\n",
    "    chunk.to_csv(f'{base_path}/kyc_information/kyc_information_{i+1:02d}.csv', index=False)\n",
    "\n",
    "# # Define schema for the DataFrame\n",
    "# schema = StructType([StructField('VerificationStatus', StringType(), True)])\n",
    "\n",
    "# # Create DataFrame with the specified schema\n",
    "# spark_df = spark.createDataFrame(kyc_information_df, schema=schema)\n",
    "\n",
    "# spark_df.write.mode(\"overwrite\").parquet(f\"{base_path}/kyc_information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TaxReport table\n",
    "tax_report_data = {\n",
    "    'ReportID': ['TAX' + str(i) for i in range(1, num_records + 1)],\n",
    "    'UserID': ['USR' + str(np.random.randint(1, num_records_user + 1)) for _ in range(num_records)],\n",
    "    'TradeID': ['SPT' + str(np.random.randint(1, num_records + 1)) for _ in range(num_records)],  # Assuming mix of trades, adjust as needed\n",
    "    'ReportDate': [fake.date_between(start_date='-2y', end_date='today') for _ in range(num_records)],\n",
    "    'ReportedAmount': [round(np.random.uniform(1000, 50000), 2) for _ in range(num_records)],\n",
    "}\n",
    "tax_report_df = pd.DataFrame(tax_report_data)\n",
    "\n",
    "# Split the DataFrame into chunks and export each one to a CSV file\n",
    "total_records = len(tax_report_df)  # Total number of records in the DataFrame\n",
    "num_files = (total_records + num_records_per_file - 1) // num_records_per_file  # Calculate the number of files needed\n",
    "\n",
    "directory = f'{base_path}/tax_report'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_idx = i * num_records_per_file\n",
    "    end_idx = start_idx + num_records_per_file\n",
    "    chunk = tax_report_df.iloc[start_idx:end_idx]\n",
    "    chunk.to_csv(f'{base_path}/tax_report/tax_report_df{i+1:02d}.csv', index=False)\n",
    "\n",
    "# # Create DataFrame with the specified schema\n",
    "# spark_df = spark.createDataFrame(tax_report_df)\n",
    "\n",
    "# spark_df.write.mode(\"overwrite\").parquet(f\"{base_path}/tax_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account table\n",
    "# account_data = {\n",
    "#     'AccountID': ['ACC' + str(i) for i in range(1, num_records + 1)],\n",
    "#     'UserID': ['USR' + str(np.random.randint(1, num_records_user + 1)) for _ in range(num_records)],\n",
    "#     'AccountType': [np.random.choice(['Main', 'Trading', 'Savings']) for _ in range(num_records)],\n",
    "#     'Balance': [round(np.random.uniform(100, 10000), 2) for _ in range(num_records)],\n",
    "#     'CurrencyID': ['CUR' + str(np.random.randint(1, 31)) for _ in range(num_records)],  # Adjusting for 30 currencies\n",
    "# }\n",
    "# accounts_df = pd.DataFrame(account_data)\n",
    "\n",
    "# PrivacyPreferences table\n",
    "privacy_preferences_data = {\n",
    "    'PrivacyPrefID': ['PRV' + str(i) for i in range(1, num_records_user + 1)],\n",
    "    'UserID': ['USR' + str(np.random.randint(1, num_records_user + 1)) for _ in range(num_records_user)],\n",
    "    'OptInMarketing': [np.random.choice([True, False]) for _ in range(num_records_user)],\n",
    "    'OptInDataSharing': [np.random.choice([True, False]) for _ in range(num_records_user)],\n",
    "}\n",
    "privacy_preferences_df = pd.DataFrame(privacy_preferences_data)\n",
    "\n",
    "# Split the DataFrame into chunks and export each one to a CSV file\n",
    "total_records = len(privacy_preferences_df)  # Total number of records in the DataFrame\n",
    "num_files = (total_records + num_records_per_file - 1) // num_records_per_file  # Calculate the number of files needed\n",
    "\n",
    "directory = f'{base_path}/privacy_preferences'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_idx = i * num_records_per_file\n",
    "    end_idx = start_idx + num_records_per_file\n",
    "    chunk = privacy_preferences_df.iloc[start_idx:end_idx]\n",
    "    chunk.to_csv(f'{base_path}/privacy_preferences/privacy_preferences_{i+1:02d}.csv', index=False)\n",
    "\n",
    "# # Create DataFrame with the specified schema\n",
    "# spark_df = spark.createDataFrame(privacy_preferences_df)\n",
    "\n",
    "# spark_df.write.mode(\"overwrite\").parquet(f\"{base_path}/privacy_preferences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stop the Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the SuspiciousActivityReport table using SQL in Python with Spark, we need to define the conditions under which a transaction or trade would be flagged as suspicious. Here are some potential scenarios:\n",
    "\n",
    "1. Large Transactions: Transactions involving a significantly large amount of funds could be flagged.\n",
    "2. Unusual Trading Patterns: Trades that deviate significantly from a user's typical trading behavior could raise suspicion.\n",
    "3. Rapid Account Activity: A sudden increase in trading activity or frequency could be indicative of suspicious behavior.\n",
    "4. Cross-Border Transactions: Transactions involving users from different countries or regions could raise concerns.\n",
    "5. Connection to Known Suspicious Entities: Users or accounts associated with previously flagged activities may warrant scrutiny.\n",
    "6. Lack of KYC Information: Transactions involving users with incomplete or unverified KYC information may be flagged.\n",
    "7. Unexplained Source of Funds: Transactions with unclear or unexplained sources of funds could be deemed suspicious.\n",
    "8. Attempted Market Manipulation: Trades aimed at manipulating market prices or creating artificial demand could be flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering DataFrames as temporary views to perform SQL queries\n",
    "users.createOrReplaceTempView(\"users\")\n",
    "spot_trades.createOrReplaceTempView(\"spot_trades\")\n",
    "otc_trades.createOrReplaceTempView(\"otc_trades\")\n",
    "futures_trades.createOrReplaceTempView(\"futures_trades\")\n",
    "\n",
    "# Generate SuspiciousActivityReport using SQL\n",
    "suspicious_reports = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CONCAT('R', id) AS ReportID,\n",
    "        UserID,\n",
    "        TradeID,\n",
    "        CURRENT_TIMESTAMP() AS ReportDate,\n",
    "        CASE \n",
    "            WHEN FiatAmount > 10000 OR CryptoAmount > 1000 THEN 'Large Transactions'\n",
    "            WHEN (OrderType = 'Buy' AND FiatAmount > 5000) OR (OrderType = 'Sell' AND FiatAmount > 3000) THEN 'Unusual Trading Patterns'\n",
    "            WHEN TIMESTAMPDIFF(DAY, Timestamp, CURRENT_TIMESTAMP()) <= 7 AND AccountType = 'Main' THEN 'Rapid Account Activity'\n",
    "            WHEN Country != 'US' AND Country != 'Canada' THEN 'Cross-Border Transactions'\n",
    "            -- Add more conditions as needed\n",
    "            ELSE 'Other Reasons'\n",
    "        END AS Reason\n",
    "    FROM (\n",
    "        SELECT \n",
    "            UserID,\n",
    "            OrderID AS TradeID,\n",
    "            FiatAmount,\n",
    "            CryptoAmount,\n",
    "            Timestamp,\n",
    "            User.AccountType,\n",
    "            COALESCE(Country, 'Unknown') AS Country,\n",
    "            OrderType\n",
    "        FROM spot_trades\n",
    "        JOIN users ON spot_trades.UserID = users.UserID\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            SellerUserID AS UserID,\n",
    "            OrderID AS TradeID,\n",
    "            FiatAmount,\n",
    "            CryptoAmount,\n",
    "            Timestamp,\n",
    "            User.AccountType,\n",
    "            COALESCE(Country, 'Unknown') AS Country,\n",
    "            OrderType\n",
    "        FROM otc_trades\n",
    "        JOIN users ON otc_trades.SellerUserID = users.UserID\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            UserID,\n",
    "            TradeID,\n",
    "            FiatAmount,\n",
    "            CryptoAmount,\n",
    "            Timestamp,\n",
    "            User.AccountType,\n",
    "            COALESCE(Country, 'Unknown') AS Country,\n",
    "            'N/A' AS OrderType\n",
    "        FROM futures_trades\n",
    "        JOIN users ON futures_trades.UserID = users.UserID\n",
    "    ) AS combined_trades\n",
    "    \"\"\")\n",
    "    \n",
    "# Show the generated SuspiciousActivityReport\n",
    "suspicious_reports.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a crypto exchange operating in Canada and the US, there are several important compliance and regulatory frameworks that need to be closely adhered to. Here are some key use cases along with the relevant compliance and regulatory requirements:\n",
    "\n",
    "1. Anti-Money Laundering (AML) and Know Your Customer (KYC) Compliance:\n",
    "\t- Use Case: Implement robust AML and KYC procedures to prevent money laundering and terrorist financing activities.\n",
    "\t- Compliance Requirements:\n",
    "\t\t- Canada: Proceeds of Crime (Money Laundering) and Terrorist Financing Act (PCMLTFA) and its associated regulations, overseen by the Financial Transactions and Reports Analysis Centre of Canada (FINTRAC). FINTRAC\n",
    "\t\t- US: Bank Secrecy Act (BSA) and Anti-Money Laundering (AML) regulations enforced by the Financial Crimes Enforcement Network (FinCEN). FinCEN\n",
    "2. Transaction Monitoring and Suspicious Activity Reporting:\n",
    "\t- Use Case: Develop algorithms to monitor transactions and identify suspicious activities for reporting.\n",
    "\t- Compliance Requirements:\n",
    "\t\t- Canada: Mandatory reporting of suspicious transactions to FINTRAC. Requirements outlined in PCMLTFA regulations.\n",
    "\t\t- US: Suspicious Activity Report (SAR) filing requirements under the BSA regulations administered by FinCEN.\n",
    "3. Regulatory Compliance for Securities:\n",
    "\t- Use Case: Ensure compliance with securities regulations for tokenized assets and digital securities trading.\n",
    "\t- Compliance Requirements:\n",
    "\t\t- Canada: Compliance with securities laws enforced by the Canadian Securities Administrators (CSA) at both federal and provincial levels. Canadian Securities Administrators\n",
    "\t\t- US: Compliance with securities laws enforced by the Securities and Exchange Commission (SEC). SEC\n",
    "4. Tax Compliance:\n",
    "\t- Use Case: Facilitate tax reporting and compliance for cryptocurrency transactions.\n",
    "\t- Compliance Requirements:\n",
    "\t\t- Canada: Compliance with Canada Revenue Agency (CRA) regulations for reporting cryptocurrency transactions for tax purposes. Canada Revenue Agency\n",
    "\t\t- US: Compliance with Internal Revenue Service (IRS) regulations for reporting cryptocurrency transactions and paying taxes. IRS\n",
    "5. Data Privacy and Security:\n",
    "\t- Use Case: Implement measures to protect customer data and ensure compliance with privacy regulations.\n",
    "\t- Compliance Requirements:\n",
    "\t\t- Canada: Compliance with the Personal Information Protection and Electronic Documents Act (PIPEDA) for handling personal data. Office of the Privacy Commissioner of Canada\n",
    "\t\t- US: Compliance with the California Consumer Privacy Act (CCPA) and other state-level privacy regulations, as well as sector-specific regulations like HIPAA for healthcare data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
